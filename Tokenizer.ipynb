{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\cheol\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package twitter_samples to\n[nltk_data]     C:\\Users\\cheol\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package twitter_samples is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\cheol\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     C:\\Users\\cheol\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# All modules that are required to import:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import requests\n",
    "import bs4\n",
    "import json\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.corpus import twitter_samples\n",
    "\n",
    "pos_tweet = twitter_samples.tokenized('positive_tweets.json')\n",
    "neg_tweet = twitter_samples.tokenized('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out the stop words\n",
    "# borrowed list of stop words from https://github.com/kavgan/stop-words/blob/master/terrier-stop.txt \n",
    "\n",
    "filepath = open(\"terrier-stop.txt\", \"r\")\n",
    "temp = filepath.read().split(\"\\n\")\n",
    "stop_words = { key : 1 for key in temp }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal of this part: Read through all positive / negative tweets, normalize and remove unnecessary words from tweets, then create actual dictionary-like to use for our dataset\n",
    "\n",
    "# Convert all complex part-of-speech to basic words\n",
    "# List of part-of-speech is in this link: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "# WordNetLemmatizer has a function lemmatize where you can convert complex part of speech words into basic forms\n",
    "# Things to consider:\n",
    "#   Remove all unnecessary words from normalized_neg_tweets / normalized_pos_tweets\n",
    "#   1. Remove mentions(starts with @)\n",
    "#   2. Remove links (starts with https:// or http:// )\n",
    "#   3. Remove punctuation (starts with ! or ?)\n",
    "#   4. Remove Stop-Words (words that do have little to no meaning and does not affect the context of the sentence) to make our dataset more concise\n",
    "# Note that we are keeping emoji (i.e. :) or :( . That is because these emojis do actually show sentiment of the text context)\n",
    "# If words are DETERMINERS (DT), COORDINATING CONJUCTIONS (CC), PREPOSITIONS (IN), PERSONAL / POSSESSIVE PRONOUNS (PRP / PRP$), or WH-PRONOUNS (WP) WH-ADVERB(WRB), we remove it (consider as Stop words)\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "normalizer = WordNetLemmatizer()\n",
    "punctuation_and_stop_words = {'!': 1, '\"': 1, '#': 1, '$': 1 ,'%': 1, '&': 1, \"'\": 1,'(': 1,')': 1,'*': 1,'+': 1,',': 1,'-': 1,'.': 1,':': 1,';': 1,'<': 1,'=': 1,'>': 1,'?': 1,'@': 1,'[': 1,']': 1,'^': 1,'_': 1,'`': 1,'{': 1,'|': 1,'}': 1,'~': 1,'https://': 1,'http://': 1}\n",
    "stop_words_final = {**stop_words, **punctuation_and_stop_words}\n",
    "\n",
    "\n",
    "def determiners(word):\n",
    "    if word in stop_words_final:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def normalize(tweet_list):\n",
    "    normalized_tweet = []\n",
    "    for tweet in tweet_list:\n",
    "        sentence = []\n",
    "        for token, tag in pos_tag(tweet):\n",
    "            # For Complex Noun words:\n",
    "            if tag.startswith('NN'):\n",
    "                new_tag = 'n'\n",
    "            # For Complex Verb words\n",
    "            elif tag.startswith('VB'):\n",
    "                new_tag = 'v'\n",
    "            # For stop-words\n",
    "            elif tag.startswith('DT') or tag.startswith('CC') or tag.startswith('IN') or tag.startswith('PRP') or tag.startswith('PRP$') or tag.startswith('WP') or tag.startswith('WRB'):\n",
    "                continue \n",
    "            # Every other words, convert them into adjective (pos = 'a')\n",
    "            else:\n",
    "                if determiners(token):\n",
    "                    new_tag = 'a'\n",
    "                else:\n",
    "                    continue\n",
    "            sentence.append(normalizer.lemmatize(token, new_tag))\n",
    "        normalized_tweet.append(sentence)\n",
    "    return normalized_tweet\n",
    "\n",
    "normalized_pos_tweets = normalize(pos_tweet)\n",
    "normalized_neg_tweets = normalize(neg_tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, store all positive / negative words into dictionary so it can be used as a guide for calculating sentiment for sentences\n",
    "\n",
    "pos_words_dict = {}\n",
    "neg_words_dict = {}\n",
    "\n",
    "# Store all words into dictionary\n",
    "for tweet in normalized_pos_tweets:\n",
    "    for word in tweet:\n",
    "        if word in pos_words_dict:\n",
    "            temp = pos_words_dict[word.lower()]\n",
    "            temp += 1\n",
    "            pos_words_dict[word.lower()] = temp\n",
    "        else:\n",
    "            pos_words_dict[word.lower()] = 1\n",
    "\n",
    "for tweet in normalized_neg_tweets:\n",
    "    for word in tweet:\n",
    "        if word in neg_words_dict:\n",
    "            temp = neg_words_dict[word.lower()]\n",
    "            temp += 1\n",
    "            neg_words_dict[word.lower()] = temp\n",
    "        else:\n",
    "            neg_words_dict[word.lower()] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all emojis and leave only roman alphabets\n",
    "pos_df = pd.DataFrame({'word': list(pos_words_dict.keys()), 'frequency': list(pos_words_dict.values())})\n",
    "cleaned_pos_df = pos_df.loc[pos_df['word'].str.isalpha()]\n",
    "\n",
    "neg_df = pd.DataFrame({'word': list(neg_words_dict.keys()), 'frequency': list(neg_words_dict.values())})\n",
    "cleaned_neg_df = neg_df.loc[neg_df['word'].str.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            word  frequency_x  frequency_y  frequency\n",
       "0             be         97.0         57.0       40.0\n",
       "1            top         10.0          6.0        4.0\n",
       "2         engage          7.0          0.0        7.0\n",
       "3         member         16.0          6.0       10.0\n",
       "4      community          2.0          1.0        1.0\n",
       "...          ...          ...          ...        ...\n",
       "9454       ahmad          0.0          1.0       -1.0\n",
       "9455      maslan          0.0          1.0       -1.0\n",
       "9456        hull          0.0          1.0       -1.0\n",
       "9457   supporter          0.0          1.0       -1.0\n",
       "9458  misserable          0.0          1.0       -1.0\n",
       "\n",
       "[9459 rows x 4 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>frequency_x</th>\n      <th>frequency_y</th>\n      <th>frequency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>be</td>\n      <td>97.0</td>\n      <td>57.0</td>\n      <td>40.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>top</td>\n      <td>10.0</td>\n      <td>6.0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>engage</td>\n      <td>7.0</td>\n      <td>0.0</td>\n      <td>7.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>member</td>\n      <td>16.0</td>\n      <td>6.0</td>\n      <td>10.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>community</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9454</th>\n      <td>ahmad</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>9455</th>\n      <td>maslan</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>9456</th>\n      <td>hull</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>9457</th>\n      <td>supporter</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n    </tr>\n    <tr>\n      <th>9458</th>\n      <td>misserable</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>9459 rows × 4 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "# merge the two dataframes into one\n",
    "merged = pd.merge(cleaned_pos_df, cleaned_neg_df, on='word', how='outer').fillna(0)\n",
    "merged['frequency'] = merged['frequency_x'] - merged['frequency_y']\n",
    "merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            word  frequency_x  frequency_y  frequency     scale\n",
       "0             be         97.0         57.0       40.0  3.133333\n",
       "1            top         10.0          6.0        4.0  1.213333\n",
       "2         engage          7.0          0.0        7.0  1.373333\n",
       "3         member         16.0          6.0       10.0  1.533333\n",
       "4      community          2.0          1.0        1.0  1.053333\n",
       "...          ...          ...          ...        ...       ...\n",
       "9454       ahmad          0.0          1.0       -1.0 -1.012270\n",
       "9455      maslan          0.0          1.0       -1.0 -1.012270\n",
       "9456        hull          0.0          1.0       -1.0 -1.012270\n",
       "9457   supporter          0.0          1.0       -1.0 -1.012270\n",
       "9458  misserable          0.0          1.0       -1.0 -1.012270\n",
       "\n",
       "[9459 rows x 5 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>frequency_x</th>\n      <th>frequency_y</th>\n      <th>frequency</th>\n      <th>scale</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>be</td>\n      <td>97.0</td>\n      <td>57.0</td>\n      <td>40.0</td>\n      <td>3.133333</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>top</td>\n      <td>10.0</td>\n      <td>6.0</td>\n      <td>4.0</td>\n      <td>1.213333</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>engage</td>\n      <td>7.0</td>\n      <td>0.0</td>\n      <td>7.0</td>\n      <td>1.373333</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>member</td>\n      <td>16.0</td>\n      <td>6.0</td>\n      <td>10.0</td>\n      <td>1.533333</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>community</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.053333</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9454</th>\n      <td>ahmad</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.012270</td>\n    </tr>\n    <tr>\n      <th>9455</th>\n      <td>maslan</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.012270</td>\n    </tr>\n    <tr>\n      <th>9456</th>\n      <td>hull</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.012270</td>\n    </tr>\n    <tr>\n      <th>9457</th>\n      <td>supporter</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.012270</td>\n    </tr>\n    <tr>\n      <th>9458</th>\n      <td>misserable</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.012270</td>\n    </tr>\n  </tbody>\n</table>\n<p>9459 rows × 5 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "# scale the frequencies of each word between 1 to 5 for positive words, -1 to -5 for negative words\n",
    "# if there is a duplicate word in both negative and positive dataset, take the difference in frequencies\n",
    "# and consider it as a positive word if the positive frequency is higher, and vice versa\n",
    "\n",
    "pos_max = merged['frequency'].max()\n",
    "neg_min = abs(merged['frequency'].min())\n",
    "\n",
    "def scaler(freq):\n",
    "\n",
    "    if freq > 0:\n",
    "        return freq * (4 / pos_max) + 1\n",
    "    elif freq < 0:\n",
    "        return freq * (4 / neg_min) - 1\n",
    "merged = merged.assign(**{'scale':merged['frequency'].apply(scaler)})\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         word  frequency_x  frequency_y  frequency  scale  is_null\n",
       "6         hey          1.0          1.0        0.0    NaN     True\n",
       "7       james          1.0          1.0        0.0    NaN     True\n",
       "8         how          1.0          1.0        0.0    NaN     True\n",
       "13     centre          2.0          2.0        0.0    NaN     True\n",
       "17       many          1.0          1.0        0.0    NaN     True\n",
       "...       ...          ...          ...        ...    ...      ...\n",
       "5888    lagos          1.0          1.0        0.0    NaN     True\n",
       "5890  kingdom          1.0          1.0        0.0    NaN     True\n",
       "5891   potato          1.0          1.0        0.0    NaN     True\n",
       "5892  hundred          1.0          1.0        0.0    NaN     True\n",
       "5903  distant          1.0          1.0        0.0    NaN     True\n",
       "\n",
       "[983 rows x 6 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>frequency_x</th>\n      <th>frequency_y</th>\n      <th>frequency</th>\n      <th>scale</th>\n      <th>is_null</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6</th>\n      <td>hey</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>james</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>how</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>centre</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>many</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>5888</th>\n      <td>lagos</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>5890</th>\n      <td>kingdom</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>5891</th>\n      <td>potato</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>5892</th>\n      <td>hundred</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>5903</th>\n      <td>distant</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n<p>983 rows × 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "merged = merged.assign(**{'is_null': merged['scale'].isnull().values})\n",
    "merged.loc[merged['is_null'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            word  frequency_x  frequency_y  frequency     scale  is_null\n",
       "0             be         97.0         57.0       40.0  3.133333    False\n",
       "1            top         10.0          6.0        4.0  1.213333    False\n",
       "2         engage          7.0          0.0        7.0  1.373333    False\n",
       "3         member         16.0          6.0       10.0  1.533333    False\n",
       "4      community          2.0          1.0        1.0  1.053333    False\n",
       "...          ...          ...          ...        ...       ...      ...\n",
       "9454       ahmad          0.0          1.0       -1.0 -1.012270    False\n",
       "9455      maslan          0.0          1.0       -1.0 -1.012270    False\n",
       "9456        hull          0.0          1.0       -1.0 -1.012270    False\n",
       "9457   supporter          0.0          1.0       -1.0 -1.012270    False\n",
       "9458  misserable          0.0          1.0       -1.0 -1.012270    False\n",
       "\n",
       "[8476 rows x 6 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n      <th>frequency_x</th>\n      <th>frequency_y</th>\n      <th>frequency</th>\n      <th>scale</th>\n      <th>is_null</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>be</td>\n      <td>97.0</td>\n      <td>57.0</td>\n      <td>40.0</td>\n      <td>3.133333</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>top</td>\n      <td>10.0</td>\n      <td>6.0</td>\n      <td>4.0</td>\n      <td>1.213333</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>engage</td>\n      <td>7.0</td>\n      <td>0.0</td>\n      <td>7.0</td>\n      <td>1.373333</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>member</td>\n      <td>16.0</td>\n      <td>6.0</td>\n      <td>10.0</td>\n      <td>1.533333</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>community</td>\n      <td>2.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.053333</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9454</th>\n      <td>ahmad</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.012270</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>9455</th>\n      <td>maslan</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.012270</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>9456</th>\n      <td>hull</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.012270</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>9457</th>\n      <td>supporter</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.012270</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>9458</th>\n      <td>misserable</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>-1.0</td>\n      <td>-1.012270</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n<p>8476 rows × 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "# decided to drop words with a total frequency of zero, since they were words that appeared the same number of times as both negative and positive words\n",
    "merged = merged.dropna()\n",
    "# Finished creating dataset for pos / neg words with scores (scale) included\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['https://www.nytimes.com/2020/12/16/technology/facebook-takes-the-gloves-off-in-feud-with-apple.html',\n",
       " 'https://www.nytimes.com/2020/12/13/business/media/apple-gawker-tim-cook.html',\n",
       " 'https://www.nytimes.com/2020/12/23/business/dealbook/trump-stimulus-veto.html',\n",
       " 'https://www.nytimes.com/2020/12/01/technology/amazon-apple-chips-intel-arm.html',\n",
       " 'https://www.nytimes.com/2020/12/17/technology/google-antitrust-monopoly.html',\n",
       " 'https://www.nytimes.com/2020/12/17/business/dealbook/tech-apple-facebook-fight.html',\n",
       " 'https://www.nytimes.com/2020/12/15/technology/big-tech-regulation-europe.html',\n",
       " 'https://www.nytimes.com/2020/12/14/technology/big-tech-lobbying-europe.html',\n",
       " 'https://www.nytimes.com/2020/12/09/technology/personaltech/amazon-halo-review.html',\n",
       " 'https://www.nytimes.com/2020/11/18/technology/apple-app-store-fee.html']"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "# For purpose of calculating execution time:\n",
    "start = time.time()\n",
    "\n",
    "# Now, retrieve articles from NYT (using API provided from NYT)\n",
    "response = requests.get(\"https://api.nytimes.com/svc/search/v2/articlesearch.json?q=apple&fq=news_desk:Business&page=0&api-key=fO0tDSRQQdU68GkuXbMjt1uA2FYImzVp\").json()\n",
    "docs = response['response']['docs']\n",
    "url_list = []\n",
    "for item in docs:\n",
    "    url_list.append(item['web_url'])\n",
    "article_list = []\n",
    "url_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to retrieve text from links and tokenize them into sentences.\n",
    "def tokenize_sentence(url_list):\n",
    "    text = []\n",
    "    title = []\n",
    "    abstract = []\n",
    "    for url in url_list:\n",
    "        time.sleep(0.1)\n",
    "        headers = {'user-agent': 'Mozilla/5.0 (X11; CrOS x86_64 8172.45.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.64 Safari/537.36'}\n",
    "        article = requests.get(url, headers = headers)\n",
    "        soup = bs4.BeautifulSoup(article.content, 'html.parser')\n",
    "        article_text_p = soup.find_all('p', attrs={'class': 'css-axufdj evys1bk0'})\n",
    "        abstract_text_p = soup.find('p', attrs={'class': 'css-w6ymp8 e1wiw3jv0'})\n",
    "        title_text_h1 = soup.find('h1', attrs={'data-test-id': 'headline'})\n",
    "        temp = []\n",
    "        title.append(title_text_h1.text)\n",
    "        abstract.append(abstract_text_p.text)\n",
    "    \n",
    "        for item in article_text_p:\n",
    "            temp.append(item.text)\n",
    "        space = ' '\n",
    "        article_text = space.join(temp)\n",
    "        text.append(article_text)\n",
    "\n",
    "    # Word Tokenization to sentences \n",
    "\n",
    "    tokenized_by_sentence = []\n",
    "    for num in range(len(text)):\n",
    "        del_quo = re.sub(\",”\", \" \", text[num])\n",
    "        del_quo_2  = re.sub(\"”\", \" \", del_quo)\n",
    "        del_quo_3 = re.sub(\"“\", \"\", del_quo_2)\n",
    "        text_token = re.split(\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|!|;|”)\\s\", del_quo_3)\n",
    "        text_token.insert(0, abstract[num])\n",
    "        text_token.insert(0, title[num])\n",
    "        tokenized_by_sentence.append(text_token)\n",
    "        \n",
    "    return tokenized_by_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenize and lametize the article from new york times\n",
    "\n",
    "def stop_word_filter(word):\n",
    "    if (word in stop_words): \n",
    "        return False\n",
    "    else: \n",
    "        return True\n",
    "\n",
    "# For filtering out empty strings\n",
    "stop_words[''] = 1\n",
    "\n",
    "def tokenizer_myself(given_articles):\n",
    "    tokenized_result = []\n",
    "    for article_iter in given_articles: \n",
    "        temp = []\n",
    "        for sentence in article_iter:\n",
    "            lowered_sentence = sentence.lower()\n",
    "            tokenized_sentence = lowered_sentence.split(\" \")\n",
    "            tokenized_sentence = list(filter(stop_word_filter, tokenized_sentence))\n",
    "            if len(tokenized_sentence) > 1:\n",
    "                temp.append(tokenized_sentence)\n",
    "        new_temp = normalize(temp)\n",
    "        tokenized_result.append(new_temp)\n",
    "    return tokenized_result\n",
    "\n",
    "tokenized_by_sentence_new = tokenizer_myself(tokenized_by_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate positivity or negativity of each sentence\n",
    "def sentence_calculator(tokenized_by_sentence_new):\n",
    "\n",
    "    articles_lst = []\n",
    "\n",
    "    hash_table = { key:1 for key in list(merged['word'])}\n",
    "\n",
    "    for article in tokenized_by_sentence_new:\n",
    "        sentence_vals = []\n",
    "        for sentence in article:\n",
    "            val = 1.0\n",
    "            for word in sentence:\n",
    "                if word in hash_table:\n",
    "                    val = val * merged.loc[merged['word'] == word]['scale'].values[0]\n",
    "            sentence_vals.append(val)\n",
    "        articles_lst.append(sentence_vals)\n",
    "    return articles_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the overall percent for the article (50% for Title and subtitle, other 50% for content)\n",
    "# Note that we are removing score 1 since those scores mean that our system did not find any pos / neg words from that sentence \n",
    "\n",
    "def filter_one(variable):\n",
    "    one_ind = 1.0\n",
    "    if variable == one_ind:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "def calculate_vals(articles_lst):\n",
    "    avg_score_article = []\n",
    "    for article in articles_lst:\n",
    "        new_article = list(filter(filter_one, article))\n",
    "        avg_score = (sum(new_article[0:2]) / 2) + (sum(new_article[2:]) / len(new_article[2:])) / 2\n",
    "        avg_score_article.append(avg_score)\n",
    "    return avg_score_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total execution time:  7.45346736907959\n"
     ]
    }
   ],
   "source": [
    "# For purpose of calculating execution time\n",
    "end = time.time()\n",
    "\n",
    "print(\"Total execution time: \", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to examine the correctness of our model, we will retrieve past stock related articles from the web and the daily stock prices of the past on that particular stock, and see how correctly our model can predict the future prices of stocks.\n",
    "# stock price: https://financialmodelingprep.com/developer/docs/#Stock-Historical-Price : Historical Daily Prices\n",
    "\n",
    "stock_endpoint = 'https://financialmodelingprep.com/api/v3/historical-price-full/'\n",
    "response = requests.get(stock_endpoint + 'AAPL' + '?apikey=70407133ea11d7284c70bbca4eee2547').json()\n",
    "type(response) == dict\n",
    "stock_df = pd.DataFrame(response['historical'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            date        open        high         low       close    adjClose  \\\n",
       "0     2020-12-30  135.580002  135.990005  133.399994  133.720001  133.720001   \n",
       "1     2020-12-29  138.050003  138.789993  134.339996  134.869995  134.869995   \n",
       "2     2020-12-28  133.990005  137.339996  133.509995  136.690002  136.690002   \n",
       "3     2020-12-24  131.320007  133.460007  131.100006  131.970001  131.970001   \n",
       "4     2020-12-23  132.160004  132.429993  130.779999  130.960007  130.960007   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "1254  2016-01-07   24.670000   25.032499   24.107500   24.112499   22.158121   \n",
       "1255  2016-01-06   25.139999   25.592501   24.967501   25.174999   23.134508   \n",
       "1256  2016-01-05   26.437500   26.462500   25.602501   25.677500   23.596279   \n",
       "1257  2016-01-04   25.652500   26.342501   25.500000   26.337500   24.202784   \n",
       "1258  2015-12-31   26.752501   26.757500   26.205000   26.315001   24.182106   \n",
       "\n",
       "           volume  unadjustedVolume   change  changePercent       vwap  \\\n",
       "0      92882124.0        92882124.0 -1.86000         -1.372  134.37000   \n",
       "1     120778200.0       120778200.0 -3.18001         -2.304  135.99999   \n",
       "2     124486200.0       124486200.0  2.70000          2.015  135.84666   \n",
       "3      54930100.0        54930100.0  0.64999          0.495  132.17667   \n",
       "4      88223700.0        88223700.0 -1.20000         -0.908  131.39000   \n",
       "...           ...               ...      ...            ...        ...   \n",
       "1254  324377600.0       324377600.0 -0.55750         -2.260   24.41750   \n",
       "1255  273829600.0       273829600.0  0.03500          0.139   25.24500   \n",
       "1256  223164000.0       223164000.0 -0.76000         -2.875   25.91417   \n",
       "1257  270597600.0       270597600.0  0.68500          2.670   26.06000   \n",
       "1258  163649200.0       163649200.0 -0.43750         -1.635   26.42583   \n",
       "\n",
       "                label  changeOverTime  \n",
       "0     December 30, 20        -0.01372  \n",
       "1     December 29, 20        -0.02304  \n",
       "2     December 28, 20         0.02015  \n",
       "3     December 24, 20         0.00495  \n",
       "4     December 23, 20        -0.00908  \n",
       "...               ...             ...  \n",
       "1254   January 07, 16        -0.02260  \n",
       "1255   January 06, 16         0.00139  \n",
       "1256   January 05, 16        -0.02875  \n",
       "1257   January 04, 16         0.02670  \n",
       "1258  December 31, 15        -0.01635  \n",
       "\n",
       "[1259 rows x 13 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>open</th>\n      <th>high</th>\n      <th>low</th>\n      <th>close</th>\n      <th>adjClose</th>\n      <th>volume</th>\n      <th>unadjustedVolume</th>\n      <th>change</th>\n      <th>changePercent</th>\n      <th>vwap</th>\n      <th>label</th>\n      <th>changeOverTime</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2020-12-30</td>\n      <td>135.580002</td>\n      <td>135.990005</td>\n      <td>133.399994</td>\n      <td>133.720001</td>\n      <td>133.720001</td>\n      <td>92882124.0</td>\n      <td>92882124.0</td>\n      <td>-1.86000</td>\n      <td>-1.372</td>\n      <td>134.37000</td>\n      <td>December 30, 20</td>\n      <td>-0.01372</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2020-12-29</td>\n      <td>138.050003</td>\n      <td>138.789993</td>\n      <td>134.339996</td>\n      <td>134.869995</td>\n      <td>134.869995</td>\n      <td>120778200.0</td>\n      <td>120778200.0</td>\n      <td>-3.18001</td>\n      <td>-2.304</td>\n      <td>135.99999</td>\n      <td>December 29, 20</td>\n      <td>-0.02304</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2020-12-28</td>\n      <td>133.990005</td>\n      <td>137.339996</td>\n      <td>133.509995</td>\n      <td>136.690002</td>\n      <td>136.690002</td>\n      <td>124486200.0</td>\n      <td>124486200.0</td>\n      <td>2.70000</td>\n      <td>2.015</td>\n      <td>135.84666</td>\n      <td>December 28, 20</td>\n      <td>0.02015</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2020-12-24</td>\n      <td>131.320007</td>\n      <td>133.460007</td>\n      <td>131.100006</td>\n      <td>131.970001</td>\n      <td>131.970001</td>\n      <td>54930100.0</td>\n      <td>54930100.0</td>\n      <td>0.64999</td>\n      <td>0.495</td>\n      <td>132.17667</td>\n      <td>December 24, 20</td>\n      <td>0.00495</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2020-12-23</td>\n      <td>132.160004</td>\n      <td>132.429993</td>\n      <td>130.779999</td>\n      <td>130.960007</td>\n      <td>130.960007</td>\n      <td>88223700.0</td>\n      <td>88223700.0</td>\n      <td>-1.20000</td>\n      <td>-0.908</td>\n      <td>131.39000</td>\n      <td>December 23, 20</td>\n      <td>-0.00908</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1254</th>\n      <td>2016-01-07</td>\n      <td>24.670000</td>\n      <td>25.032499</td>\n      <td>24.107500</td>\n      <td>24.112499</td>\n      <td>22.158121</td>\n      <td>324377600.0</td>\n      <td>324377600.0</td>\n      <td>-0.55750</td>\n      <td>-2.260</td>\n      <td>24.41750</td>\n      <td>January 07, 16</td>\n      <td>-0.02260</td>\n    </tr>\n    <tr>\n      <th>1255</th>\n      <td>2016-01-06</td>\n      <td>25.139999</td>\n      <td>25.592501</td>\n      <td>24.967501</td>\n      <td>25.174999</td>\n      <td>23.134508</td>\n      <td>273829600.0</td>\n      <td>273829600.0</td>\n      <td>0.03500</td>\n      <td>0.139</td>\n      <td>25.24500</td>\n      <td>January 06, 16</td>\n      <td>0.00139</td>\n    </tr>\n    <tr>\n      <th>1256</th>\n      <td>2016-01-05</td>\n      <td>26.437500</td>\n      <td>26.462500</td>\n      <td>25.602501</td>\n      <td>25.677500</td>\n      <td>23.596279</td>\n      <td>223164000.0</td>\n      <td>223164000.0</td>\n      <td>-0.76000</td>\n      <td>-2.875</td>\n      <td>25.91417</td>\n      <td>January 05, 16</td>\n      <td>-0.02875</td>\n    </tr>\n    <tr>\n      <th>1257</th>\n      <td>2016-01-04</td>\n      <td>25.652500</td>\n      <td>26.342501</td>\n      <td>25.500000</td>\n      <td>26.337500</td>\n      <td>24.202784</td>\n      <td>270597600.0</td>\n      <td>270597600.0</td>\n      <td>0.68500</td>\n      <td>2.670</td>\n      <td>26.06000</td>\n      <td>January 04, 16</td>\n      <td>0.02670</td>\n    </tr>\n    <tr>\n      <th>1258</th>\n      <td>2015-12-31</td>\n      <td>26.752501</td>\n      <td>26.757500</td>\n      <td>26.205000</td>\n      <td>26.315001</td>\n      <td>24.182106</td>\n      <td>163649200.0</td>\n      <td>163649200.0</td>\n      <td>-0.43750</td>\n      <td>-1.635</td>\n      <td>26.42583</td>\n      <td>December 31, 15</td>\n      <td>-0.01635</td>\n    </tr>\n  </tbody>\n</table>\n<p>1259 rows × 13 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}